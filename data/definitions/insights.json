{
  "model_insights": {
    "layer-wise-scaling": {
      "name": "Layer-wise Scaling",
      "description": "Non-uniform allocation of parameters in the transformer layer. Adjusted number of attention heads and FFN multiplier in each transformer layer.",
      "url": "https://arxiv.org/abs/2404.14619",
      "color": "teal-800"
    },
    "flash-attention": {
      "name": "Flash Attention",
      "description": "Flash Attention is an attention algorithm used to reduce the memory bottleneck in transformers.",
      "url": "https://arxiv.org/abs/2205.14135",
      "color": "yellow-300"
    }
  },
  "companies": {
    "apple": {
      "name": "Apple",
      "url": "https://machinelearning.apple.com/",
      "logo": "apple.svg"
    },
    "qwenlm": {
      "name": "Qwen",
      "url": "https://qwenlm.github.io/",
      "logo": "qwen.svg"
    },
    "01.ai": {
      "name": "01.AI",
      "url": "https://www.01.ai/",
      "logo": "01-ai.webp"
    }
  }
}